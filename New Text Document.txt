import torch
import cloudpickle
from torch.utils.data import Dataset
from mingpt.model import GPT
from mingpt.trainer import Trainer
import pandas as pd
import fire


class Block(torch.nn.Module):
    def __init__(self, i):
        super().__init__()
        self.linear1 = torch.nn.Linear(512, 512)
        self.relu = torch.nn.ReLU()
        self.linear2 = torch.nn.Linear(512, 512)

    def forward(self, x):
        x = self.linear1(x)
        x = self.relu(x)
        x = self.linear2(x)
        return x


class MyGPT(GPT):
    def __init__(self, config):
        super().__init__(config)
        self.blocks = []
        for i in range(config.num_blocks):
            self.blocks.append(Block(i))
        self.num_blocks = config.num_blocks

    def forward(self, x):
        for block in self.blocks:
            x = block(x)
        return x


class MyDataset(Dataset):
    def __init__(self, filepath, model):
        self.data = pd.read_csv(filepath)
        self.data['listed_in'] = pd.Categorical(self.data['listed_in'])
        self.model = model

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        text = self.data.loc[idx, 'description']
        label = self.data['listed_in'].iloc[idx]
        text_tokenized = self.model.tokenizer.encode(text)
        label_tensor = torch.tensor(label, dtype=torch.long)
        return text_tokenized, label_tensor


def train_model(fp, save_path):
    
    model_config = GPT.get_default_config()
    model_config.model_type = 'gpt2'
    model_config.vocab_size = 50257
    model_config.block_size = 1024
    model_config.num_blocks = 12
    config = Trainer.get_default_config()
    config.learning_rate = 5e-4
    config.max_iters = 1000
    config.batch_size = 32
    config.num_blocks = model_config.num_blocks
    
    model = MyGPT(model_config)

    train_dataset = MyDataset(fp, model)

    train_config = Trainer.get_default_config()
    train_config.learning_rate = 5e-4
    train_config.max_iters = 1000
    train_config.batch_size = 32

    trainer = Trainer(train_config, model, train_dataset)
    trainer.run()

    # Save the trained model using cloudpickle
    with open(save_path, 'wb') as f:
        cloudpickle.dump(model, f)


def load_model(model_path):
    # Load the trained model using cloudpickle
    with open(model_path, 'rb') as f:
        model = cloudpickle.load(f)
    model.eval()
    return model


if __name__ == "__main__":
    fire.Fire({
        'train_model': train_model,
        'load_model': load_model
    })
